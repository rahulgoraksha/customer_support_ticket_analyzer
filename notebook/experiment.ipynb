{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be615ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng') # This is the specific english model\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker') # New addition\n",
    "nltk.download('words')             # New addition\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/customer_support_tickets_1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de96a70",
   "metadata": {},
   "source": [
    "Load and inspect the dataset (CSV format). Identify\tmissing\tvalues,\tduplicates, and\tcolumn types.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated() # Check for duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # Check data types of each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93385ed3",
   "metadata": {},
   "source": [
    "Perform\tdeep text cleaning on 'message_body': remove emojis, HTML, repeated\tpunctuation, extra spaces,and fix common spelling errors.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS IS THE CORRECTED DEEP TEXT CLEANING CELL\n",
    "#\n",
    "\n",
    "def deep_text_cleaning(text):\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    html_pattern = re.compile(\"<.*?>\")\n",
    "    text = html_pattern.sub(r\"\", text)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Correct spelling - this can be slow, apply with caution on large datasets\n",
    "    # For this project, it's fine.\n",
    "    try:\n",
    "        text = str(TextBlob(text).correct())\n",
    "    except Exception:\n",
    "        # Pass if TextBlob fails on a specific text\n",
    "        pass\n",
    "\n",
    "    return text\n",
    "\n",
    "# The key line: Ensure this creates the column the next cell needs\n",
    "df[\"cleaned_message_body\"] = df[\"message_body\"].apply(deep_text_cleaning)\n",
    "df[[\"message_body\", \"cleaned_message_body\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the HTLM tags\n",
    "def remove_html_tags(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "df['message_body'] = df['message_body'].apply(remove_html_tags)\n",
    "df['message_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove repeated punctuation and special characters \n",
    "def remove_special_characters(text):    \n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "df['message_body'] = df['message_body'].apply(remove_special_characters)\n",
    "df['message_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72103b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove space extra spaces\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "df['message_body'] = df['message_body'].apply(remove_extra_spaces)\n",
    "df['message_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc9dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix spelling mistakes\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "df['message_body'] = df['message_body'].apply(correct_spelling)\n",
    "df['message_body'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f8f05",
   "metadata": {},
   "source": [
    "Perform text preprocessing — apply tokenization, lowercasing, lemmatization, and POS tagging using spaCy or NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2a4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tokenisation\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "df['tokenized_message'] = df['message_body'].apply(tokenize_text)\n",
    "df[['message_body', 'tokenized_message']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing the text\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "df['message_body'] = df['message_body'].apply(lowercase_text)\n",
    "df['message_body'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e76325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "df['lemmatized_message'] = df['message_body'].apply(lemmatize_text)\n",
    "df[['message_body', 'lemmatized_message']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df9f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagging\n",
    "def pos_tagging(text): \n",
    "    tokens = word_tokenize(text)\n",
    "    return pos_tag(tokens)\n",
    "df['pos_tagged_message'] = df['message_body'].apply(pos_tagging)\n",
    "df[['message_body', 'pos_tagged_message']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf4196",
   "metadata": {},
   "source": [
    "Extract key entities using Named Entity Recognition (NER) to identify product names, user names, locations, and issue types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_entities_nltk(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Apply Part-of-Speech (POS) tagging\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    # Apply Named Entity Recognition (NER) chunking\n",
    "    chunks = nltk.ne_chunk(pos_tags, binary=False)\n",
    "\n",
    "    entities = {\n",
    "        \"user_names\": [],\n",
    "        \"locations\": []\n",
    "    }\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'PERSON':\n",
    "                entities['user_names'].append(' '.join(c[0] for c in chunk))\n",
    "            elif chunk.label() == 'GPE': # GPE is the label for locations\n",
    "                entities['locations'].append(' '.join(c[0] for c in chunk))\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['key_entities'] = df['cleaned_message_body'].apply(extract_entities_nltk)\n",
    "df[['cleaned_message_body', 'key_entities']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0425a3f1",
   "metadata": {},
   "source": [
    "Classify each message into categories such as Complaint, Bug Report, Feature Request, Praise, etc., using either traditional ML models or transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ea9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify each message into categories\n",
    "def classify_message(text):\n",
    "    text = text.lower()\n",
    "    if \"billing\" in text or \"invoice\" in text or \"payment\" in text:\n",
    "        return \"Complaint\"\n",
    "    elif \"technical\" in text or \"error\" in text or \"bug\" in text:\n",
    "        return \"Bug Report\"\n",
    "    elif \"account\" in text or \"login\" in text or \"password\" in text:\n",
    "        return \"Feature Request\"\n",
    "    else:\n",
    "        return \"Praise\"\n",
    "    \n",
    "df['message_category'] = df['message_body'].apply(classify_message)\n",
    "df[['message_body', 'message_category']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS IS THE CORRECTED TRANSFORMER NER CELL\n",
    "#\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# The key change is adding \"dbmdz/\" to the model name\n",
    "ner_pipeline = pipeline(\"ner\", model='dbmdz/bert-large-cased-finetuned-conll03-english', grouped_entities=True)\n",
    "\n",
    "def transformer_ner(text):\n",
    "    # It's good practice to handle empty strings\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    return ner_pipeline(text)\n",
    "\n",
    "# Let's apply it to a new column to test\n",
    "# Note: This will be slow as it runs on the CPU by default.\n",
    "# It's better to run this on a small sample if you're just testing.\n",
    "df['transformer_ner_entities'] = df['cleaned_message_body'].head().apply(transformer_ner) # Using .head() to run on first 5 rows only\n",
    "\n",
    "df[['cleaned_message_body', 'transformer_ner_entities']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362161a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# from transformers import torchpipeline  <-- DELETE THIS LINE\n",
    "\n",
    "# The model name is correct now\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
    "\n",
    "def transformer_ner(text):\n",
    "    # It's good practice to handle empty strings\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    return ner_pipeline(text)\n",
    "\n",
    "# Let's apply it to a new column to test\n",
    "# Note: Using .head(10) to run only on the first 10 rows to save time during testing\n",
    "df['transformer_ner_entities'] = df['cleaned_message_body'].head(10).apply(transformer_ner)\n",
    "\n",
    "df[['cleaned_message_body', 'transformer_ner_entities']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a60c92",
   "metadata": {},
   "source": [
    "Summarize lengthy messages — for messages exceeding 100 words, apply text summarization using pretrained models like T5 or BART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799625ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# REPLACE YOUR ENTIRE TEXT SUMMARIZATION CELL WITH THIS\n",
    "#\n",
    "\n",
    "# 1. Check if a CUDA-enabled GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"GPU is available. Using CUDA.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "\n",
    "# 2. Create the summarization pipeline and assign it to the detected device\n",
    "# The `device=device` argument is the key change that enables the GPU.\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", device=device)\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Only summarize longer messages to save time\n",
    "    if len(text.split()) > 100:\n",
    "        summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    else:\n",
    "        return \"Message is short, no summary needed.\"\n",
    "\n",
    "# This will now run much faster on the GPU for long messages\n",
    "df['summary'] = df['cleaned_message_body'].apply(summarize_text)\n",
    "\n",
    "# To see the full summary text in the output\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df[['cleaned_message_body', 'summary']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726a96d",
   "metadata": {},
   "source": [
    "Message Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa8882c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'processed_message'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\customer_support_ticket_analyzer\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'processed_message'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Feature and target variables\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Ensure the 'processed_message' column was created in a previous step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m X = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed_message\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mtrue_category\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Split data into training and testing sets\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\customer_support_ticket_analyzer\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\customer_support_ticket_analyzer\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'processed_message'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# THIS IS THE MESSAGE CLASSIFICATION CELL\n",
    "# Make sure this cell is present and has been run before creating the final CSV.\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Feature and target variables\n",
    "# Ensure the 'processed_message' column was created in a previous step\n",
    "X = df['processed_message']\n",
    "y = df['true_category']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression Model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# (Optional) Check Model Accuracy\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "print(f\"Model Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "# The key line that creates the missing column\n",
    "df_tfidf = tfidf_vectorizer.transform(df['processed_message'])\n",
    "df['predicted_category'] = model.predict(df_tfidf)\n",
    "\n",
    "df[['true_category', 'predicted_category']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeede67f",
   "metadata": {},
   "source": [
    "Generate an output CSV file containing the cleaned text, predicted category, extracted entities, and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS IS THE CORRECTED FINAL CELL TO GENERATE THE CSV OUTPUT\n",
    "#\n",
    "\n",
    "# Select only the required columns for the final deliverable\n",
    "final_columns = [\n",
    "    'message_id',\n",
    "    'cleaned_message_body',\n",
    "    'predicted_category',\n",
    "    'key_entities',\n",
    "    'summary'\n",
    "]\n",
    "\n",
    "# Create the final DataFrame\n",
    "output_df = df[final_columns].copy()\n",
    "\n",
    "# Convert the 'key_entities' dictionary to a more readable string format for the CSV\n",
    "output_df['key_entities'] = output_df['key_entities'].astype(str)\n",
    "\n",
    "# Define the output path and save the CSV\n",
    "output_file_path = \"processed_customer_support_tickets.csv\"\n",
    "output_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Final CSV file generated successfully at: {output_file_path}\")\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8989c",
   "metadata": {},
   "source": [
    "(Optional) Develop an interactive app using Streamlit or Gradio, with filters for category, city, or keyword-based search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

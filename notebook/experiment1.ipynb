{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dd52e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data... (This may take a moment)\n",
      "NLTK data download complete.\n",
      "\n",
      "Step 1: Loading and Inspecting Data...\n",
      "Data loaded successfully. Here's a preview:\n",
      "  message_id          user_name     location  \\\n",
      "0      M0001        Ryan Obrien  North Billy   \n",
      "1      M0002      Jamie Salazar   South Kari   \n",
      "2      M0003    Clinton Wallace   Port Paige   \n",
      "3      M0004  Christopher Olsen    Jeanville   \n",
      "4      M0005     Alex Alexander    West Gail   \n",
      "\n",
      "                                     email_subject  \\\n",
      "0                    Law from traditional now Mrs.   \n",
      "1  Require billion probably cut raise include now.   \n",
      "2                        South maintain year firm.   \n",
      "3                              Fill personal fire.   \n",
      "4               Term authority offer feeling than.   \n",
      "\n",
      "                                        message_body           created_at  \\\n",
      "0  Reflect available century join outside. i cant...  2025-09-01 06:10:59   \n",
      "1  Try cause behind single project. Sport sound c...  2025-06-16 04:58:19   \n",
      "2  While travel major strong pull. Us history lig...  2025-08-17 14:54:02   \n",
      "3  Can nothing force move free body stand approac...  2025-10-15 06:27:28   \n",
      "4  Control skin fall. Left worker ready take prop...  2025-05-14 22:35:51   \n",
      "\n",
      "     true_category  \n",
      "0  Feature Request  \n",
      "1            Other  \n",
      "2  Feature Request  \n",
      "3           Praise  \n",
      "4           Praise  \n",
      "\n",
      "Checking for missing values:\n",
      "message_id       0\n",
      "user_name        0\n",
      "location         0\n",
      "email_subject    0\n",
      "message_body     0\n",
      "created_at       0\n",
      "true_category    0\n",
      "dtype: int64\n",
      "\n",
      "Checking for duplicates:\n",
      "Found 0 duplicate rows.\n",
      "\n",
      "Checking column types:\n",
      "message_id       object\n",
      "user_name        object\n",
      "location         object\n",
      "email_subject    object\n",
      "message_body     object\n",
      "created_at       object\n",
      "true_category    object\n",
      "dtype: object\n",
      "\n",
      "Step 2: Performing Deep Text Cleaning...\n",
      "Deep text cleaning complete.\n",
      "\n",
      "Step 3: Preprocessing text for ML model...\n",
      "Text preprocessing for ML complete.\n",
      "\n",
      "Step 4: Extracting Named Entities...\n",
      "Named Entity Recognition complete.\n",
      "\n",
      "Step 5: Classifying messages...\n",
      "Message classification complete. Model Accuracy: 0.14\n",
      "\n",
      "Step 6: Summarizing long messages...\n",
      "GPU is available. Using CUDA for summarization.\n",
      "Summarization complete.\n",
      "\n",
      "Step 7: Generating final CSV file...\n",
      "\n",
      "SUCCESS! Final CSV file generated successfully at: ../data/processed_customer_support_tickets.csv\n",
      "Here is a preview of the final output:\n",
      "  message_id                               cleaned_message_body  \\\n",
      "0      M0001  reflect available century join outside i cant ...   \n",
      "1      M0002  try cause behind single project sport sound cl...   \n",
      "2      M0003  while travel major strong pull us history ligh...   \n",
      "3      M0004  can nothing force move free body stand approac...   \n",
      "4      M0005  control skin fall left worker ready take prope...   \n",
      "\n",
      "  predicted_category                         key_entities  \n",
      "0    Feature Request  {'user_names': [], 'locations': []}  \n",
      "1          Complaint  {'user_names': [], 'locations': []}  \n",
      "2             Praise  {'user_names': [], 'locations': []}  \n",
      "3      Account Issue  {'user_names': [], 'locations': []}  \n",
      "4             Praise  {'user_names': [], 'locations': []}  \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell 1: All Imports and NLTK Downloads\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download all necessary NLTK data packages one time\n",
    "print(\"Downloading NLTK data... (This may take a moment)\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "print(\"NLTK data download complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 2: Load and Inspect Data\n",
    "# =============================================================================\n",
    "print(\"\\nStep 1: Loading and Inspecting Data...\")\n",
    "df = pd.read_csv(\"../data/customer_support_tickets_1.csv\")\n",
    "print(\"Data loaded successfully. Here's a preview:\")\n",
    "print(df.head())\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nChecking for duplicates:\")\n",
    "print(f\"Found {df.duplicated().sum()} duplicate rows.\")\n",
    "print(\"\\nChecking column types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 3: Deep Text Cleaning\n",
    "# =============================================================================\n",
    "print(\"\\nStep 2: Performing Deep Text Cleaning...\")\n",
    "\n",
    "def deep_text_cleaning(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\" u\"\\U0001F300-\\U0001F5FF\" u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" u\"\\U00002702-\\U000027B0\" u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Correct basic spelling errors (optional, can be slow)\n",
    "    # try:\n",
    "    #     text = str(TextBlob(text).correct())\n",
    "    # except Exception:\n",
    "    #     pass\n",
    "    return text\n",
    "\n",
    "df['cleaned_message_body'] = df['message_body'].apply(deep_text_cleaning)\n",
    "print(\"Deep text cleaning complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 4: Text Preprocessing (Lemmatization, Stopword Removal)\n",
    "# THIS CELL CREATES THE 'processed_message' COLUMN\n",
    "# =============================================================================\n",
    "print(\"\\nStep 3: Preprocessing text for ML model...\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_for_ml(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "df['processed_message'] = df['cleaned_message_body'].apply(preprocess_for_ml)\n",
    "print(\"Text preprocessing for ML complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 5: Named Entity Recognition (NER)\n",
    "# =============================================================================\n",
    "print(\"\\nStep 4: Extracting Named Entities...\")\n",
    "\n",
    "def extract_entities_nltk(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    chunks = nltk.ne_chunk(pos_tags, binary=False)\n",
    "    entities = {\"user_names\": [], \"locations\": []}\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'PERSON':\n",
    "                entities['user_names'].append(' '.join(c[0] for c in chunk))\n",
    "            elif chunk.label() == 'GPE':\n",
    "                entities['locations'].append(' '.join(c[0] for c in chunk))\n",
    "    return entities\n",
    "\n",
    "df['key_entities'] = df['cleaned_message_body'].apply(extract_entities_nltk)\n",
    "print(\"Named Entity Recognition complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 6: Message Classification\n",
    "# =============================================================================\n",
    "print(\"\\nStep 5: Classifying messages...\")\n",
    "\n",
    "# Feature and target variables\n",
    "X = df['processed_message']\n",
    "y = df['true_category']\n",
    "\n",
    "# Split, Train, and Predict\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Apply model to the entire dataset to create the 'predicted_category' column\n",
    "df_tfidf = tfidf_vectorizer.transform(df['processed_message'])\n",
    "df['predicted_category'] = model.predict(df_tfidf)\n",
    "print(f\"Message classification complete. Model Accuracy: {accuracy_score(y_test, model.predict(X_test_tfidf)):.2f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 7: Text Summarization\n",
    "# =============================================================================\n",
    "print(\"\\nStep 6: Summarizing long messages...\")\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"GPU is available. Using CUDA for summarization.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"GPU not available. Using CPU for summarization.\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", device=device)\n",
    "\n",
    "def summarize_text(text):\n",
    "    if len(text.split()) > 100:\n",
    "        try:\n",
    "            summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "            return summary[0]['summary_text']\n",
    "        except Exception as e:\n",
    "            return f\"Summarization failed: {e}\"\n",
    "    else:\n",
    "        return \"Message is short, no summary needed.\"\n",
    "\n",
    "df['summary'] = df['cleaned_message_body'].apply(summarize_text)\n",
    "print(\"Summarization complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Cell 8: Create Final CSV Output\n",
    "# =============================================================================\n",
    "print(\"\\nStep 7: Generating final CSV file...\")\n",
    "\n",
    "final_columns = [\n",
    "    'message_id',\n",
    "    'cleaned_message_body',\n",
    "    'predicted_category',\n",
    "    'key_entities',\n",
    "]\n",
    "output_df = df[final_columns].copy()\n",
    "output_df['key_entities'] = output_df['key_entities'].astype(str)\n",
    "output_file_path = \"../data/processed_customer_support_tickets.csv\"\n",
    "output_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"\\nSUCCESS! Final CSV file generated successfully at: {output_file_path}\")\n",
    "print(\"Here is a preview of the final output:\")\n",
    "print(output_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
